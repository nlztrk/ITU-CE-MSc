{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from blg561.layer.layer import MaxPool2d, Dropout, BatchNorm\n",
    "from blg561.checks import rel_error, grad_check\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are going to implement regularization techniques widely used until recently in convolutional networks such as **Max Pooling** and **Dropout**\n",
    "\n",
    "Find `MaxPool2d`, `BatchNorm`, `Dropout` classes in **layer/layer.py** and complete the implementation of `forward` and `backward` methods for both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dropout layer\n",
    "\n",
    "As we covered in the class, dropout is a well-known regularization technique for preventing overfitting of neural networks. What dropout does is basically zeroing out of some outputs of hidden layers at random. We recommend you to multiply the dropout factor with outputs in forward pass as it is done in common implementations. Recall that this is called **Inverted Dropout**.\n",
    "\n",
    "For more information on dropout, you can check the paper below.\n",
    "\n",
    "**Improving neural networks by preventing co-adaptation of feature detectors**, Hinton et al.\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate is:  0.3\n",
      "Percent of how much of input is zeroed out in training  0.300096\n",
      "Percent of how much of input is zeroed out in testing 0.0\n",
      "Dropout rate is:  0.5\n",
      "Percent of how much of input is zeroed out in training  0.499705\n",
      "Percent of how much of input is zeroed out in testing 0.0\n",
      "Dropout rate is:  0.8\n",
      "Percent of how much of input is zeroed out in training  0.800321\n",
      "Percent of how much of input is zeroed out in testing 0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "\n",
    "x = np.random.randn(1000, 1000) + 1773\n",
    "for p in [0.3, 0.5, 0.8]:\n",
    "    dropout = Dropout(p=p)\n",
    "    dropout.mode = 'train'\n",
    "    out = dropout.forward(x)\n",
    "    dropout.mode = 'test'\n",
    "    out_test = dropout.forward(x)\n",
    "\n",
    "    print('Dropout rate is: ', p)\n",
    "    print('Percent of how much of input is zeroed out in training ', (out == 0).mean())\n",
    "    print('Percent of how much of input is zeroed out in testing', (out_test == 0).mean())\n",
    "\n",
    "# You can check wheter your implemention is true or not by looking at the percent of outputs set to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on dx  5.445605283989963e-11\n"
     ]
    }
   ],
   "source": [
    "dropout = Dropout(p=0.8)\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "\n",
    "out = dropout.forward(x,seed=1773)\n",
    "dx = dropout.backward(dout)\n",
    "dx_num = grad_check(lambda xx: dropout.forward(xx, seed=1773), x, dout)\n",
    "\n",
    "print('Error on dx ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. MaxPool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "Implement the forward pass for the max-pooling operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  3.8333348231602604e-07\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 7, 7)\n",
    "x = np.linspace(-0.3, 0.3, num=np.prod(x_shape)).reshape(x_shape)\n",
    "maxPool = MaxPool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "out = maxPool.forward(x)\n",
    "\n",
    "correct_out = np.array([[[[-0.26723549, -0.26313993, -0.25904437],\n",
    "   [-0.23856655, -0.23447099 ,-0.23037543],\n",
    "   [-0.20989761, -0.20580205, -0.20170648],],\n",
    "\n",
    "  [[-0.1668942,  -0.16279863, -0.15870307],\n",
    "   [-0.13822526, -0.13412969, -0.13003413],\n",
    "   [-0.10955631, -0.10546075, -0.10136519],],\n",
    "\n",
    "  [[-0.0665529,  -0.06245734, -0.05836177],\n",
    "   [-0.03788396, -0.0337884,  -0.02969283],\n",
    "   [-0.00921502, -0.00511945, -0.00102389],],],\n",
    "\n",
    "\n",
    " [[[ 0.0337884 ,  0.03788396,  0.04197952],\n",
    "   [ 0.06245734,  0.0665529,   0.07064846],\n",
    "   [ 0.09112628,  0.09522184,  0.09931741],],\n",
    "\n",
    "  [[ 0.13412969,  0.13822526,  0.14232082],\n",
    "   [ 0.16279863,  0.1668942,   0.17098976],\n",
    "   [ 0.19146758,  0.19556314,  0.1996587 ],],\n",
    "\n",
    "  [[ 0.23447099,  0.23856655,  0.24266212],\n",
    "   [ 0.26313993,  0.26723549,  0.27133106],\n",
    "   [ 0.29180887,  0.29590444,  0.3       ],],],])\n",
    "\n",
    "err = rel_error(out, correct_out)\n",
    "print('Error: ', rel_error(out, correct_out))\n",
    "assert err < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "Implement the backward pass for the max-pooling operation. You only need to pass the gradient from the maximum of the filter kernel position, the rest should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.275631092305069e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 1, 8, 8)\n",
    "dout = np.random.randn(10, 1, 4, 4)\n",
    "max_pool = MaxPool2d(pool_height=2, pool_width=2, stride=2)\n",
    "dx_num = grad_check(lambda x: max_pool.forward(x), x, dout)\n",
    "\n",
    "out = max_pool.forward(x)\n",
    "dx = max_pool.backward(dout)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "First read and understand the paper:\n",
    "\n",
    "S. Ioffe, C. Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Implement the forward pass for the Batch Normalization technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using batchnorn\n",
      "  mean of each feature/channel:  [2.91251878 2.39092788 2.84535145]\n",
      "  stds of each feature/channel  [4.3901125  3.44430624 4.22973534]\n",
      " Stats after batch normalization with gamma=1, beta=0\n",
      "  mean:  [-1.59872116e-16 -2.63122857e-16 -2.27595720e-16]\n",
      "  std:  [0.99999974 0.99999958 0.99999972]\n",
      " Stats after batch normalization with arbitirary parameters\n",
      "  mean:  [7. 3. 4.]\n",
      "  std:  [0.99999974 1.99999916 2.99999916]\n"
     ]
    }
   ],
   "source": [
    "# You should understand how the gamma and beta parameters affect to the output\n",
    "\n",
    "# An example of a single hidden layer with ReLU activation.\n",
    "np.random.seed(1773)\n",
    "N, D1, D2 = 200, 50, 3,\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "a = np.maximum(0, X.dot(W1))\n",
    "\n",
    "bn1 = BatchNorm(D2)\n",
    "\n",
    "print('Without using batchnorn')\n",
    "print('  mean of each feature/channel: ', a.mean(axis=0))\n",
    "print('  stds of each feature/channel ', a.std(axis=0))\n",
    "\n",
    "\n",
    "print(' Stats after batch normalization with gamma=1, beta=0')\n",
    "normalized = bn1.forward(a)\n",
    "print('  mean: ', normalized.mean(axis=0))\n",
    "print('  std: ', normalized.std(axis=0))\n",
    "\n",
    "\n",
    "bn1.gamma = np.array([1.0, 2.0, 3.0])\n",
    "bn1.beta = np.array([7, 3, 4])\n",
    "normalized  = bn1.forward(a)\n",
    "print(' Stats after batch normalization with arbitirary parameters')\n",
    "print('  mean: ', normalized.mean(axis=0))\n",
    "print('  std: ', normalized.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass**\n",
    "Implement the forward pass for the Batch Normalization technique. Follow the paper by Ioffe et al. for the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  5.041038273846015e-08\n",
      "dgamma error:  2.7072855936850803e-11\n",
      "dbeta error:  3.2759047930723823e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(1773)\n",
    "N, D = 20, 6\n",
    "x = 3 * np.random.randn(N, D) + 13\n",
    "\n",
    "bn1 = BatchNorm(D)\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "fx = lambda x: bn1.forward(x, gamma=gamma, beta=beta)\n",
    "fg = lambda a: bn1.forward(x, gamma=a, beta=beta)\n",
    "fb = lambda b: bn1.forward(x, gamma=gamma, beta=b)\n",
    "\n",
    "dx_num = grad_check(fx, x, dout)\n",
    "da_num = grad_check(fg, gamma.copy(), dout)\n",
    "db_num = grad_check(fb, beta.copy(), dout)\n",
    "\n",
    "bn1.forward(x, gamma=gamma, beta=beta)\n",
    "dx, dgamma, dbeta = bn1.backward(dout)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
