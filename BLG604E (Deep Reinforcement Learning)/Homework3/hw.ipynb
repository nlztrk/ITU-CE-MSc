{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "The general organization of the homework is given below.\n",
    "\n",
    "- pg\n",
    "    - reinforce\n",
    "        - model\n",
    "        - box2d\n",
    "    - a2c\n",
    "        - model\n",
    "        - vecenv\n",
    "        - box2d\n",
    "        - pong\n",
    "    - common\n",
    "\n",
    "In this homework, we will be implementing REINFORCE and A2C agents and run these agents in CartPole and LunarLander environments. Moreover, there will be a Pong run with A2C agent (luckily, GPU is not a must with PG agents).\n",
    "\n",
    "#### Running\n",
    "\n",
    "Each experiment will be trained from scratch with a different seed 5 times to have a good understanding of the stochasticity involved in the training. You can run your experiments with command-line arguments from the ipython notebook as shown below or using a bash script. **Please do not change the default values of the arguments!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/a2c/box2d.py --nenv 16 > logs/a2c/lunarlander/lunar_1.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then parse the CSV file using the parser method to use it in visualizations. Note that, except the pong experiment where you will be running a single time, all the experiments must have 5 different runs (with the same arguments and code but with different seeds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "When you are done with experiments, you can plot the statistics. We are interested to see how much variation exists in the training. **You need to keep log files for the submission!** Do not rely on the plots in the Ipython notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, train_rewards, title):\n",
    "    \"\"\" Plot the statistics.\n",
    "        Arguments:\n",
    "            - x: Shared x axis of (N) dim array\n",
    "            - train_rewards: (5, N) dim array \n",
    "    \"\"\"\n",
    "    if len(train_rewards.shape) != 2:\n",
    "        raise ValueError(\"train_rewards must be 2 dimensional\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Mean Episodic reward\")\n",
    "    plt.plot(\n",
    "        x,\n",
    "        np.percentile(train_rewards, 50, axis=0),\n",
    "        label=\"InterQuartile Range\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        x,\n",
    "        np.percentile(train_rewards, 25, axis=0),\n",
    "        np.percentile(train_rewards, 75, axis=0),\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pg.common import PrintWriter\n",
    "import numpy as np\n",
    "\n",
    "# Assumeing that you have the files (lunar_1, lunar_2, ..., lunar_5)\n",
    "runs = [PrintWriter.parse(\"logs/a2c/lunarlander/lunar_{}.csv\".format(i)) for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards = np.array([r[\" Reward\"] for r in runs])\n",
    "x_axis = np.array(runs[0][\"Iteration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards[0] *= 0.8\n",
    "train_rewards[1] *= 0.9\n",
    "train_rewards[2] *= 1.1\n",
    "train_rewards[3] *= 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_axis, train_rewards, \"A2C LunarLander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce Implementation\n",
    "\n",
    "The implementation of REINFORCE is pretty simple. There are two python files and we will begin with filling the \"model.py\". Since you are familiar with REINFORCE, we can start implementing the ```learn``` method in the ```Reinforce``` class. It excepts ```args```, ```opt```, and ```env``` as parameters. We will be looping as many episodes as stated in the argument ```args.n_episodes```. Note that, if an episode cannot terminate in ```args.max_episode_len``` many steps, we will terminate it manually.\n",
    "\n",
    "We need to obtain a rollout that consists of a list of transitions. Each transition in the rollout needs to include the log probability of the selected (and performed) action and the reward. Simply, call the ```policynet``` and create a Categorical distribution. Then, take a sample from the distribution and step the environment with it. Now, we just need to obtain the log probability of taking that particular action. If you implemented everything correctly, the rollout list that is just initialized above the rollout loop is filled with transitions of log probability and reward (Number of transitions must be equal to the length of the episode).\n",
    "\n",
    "Since we have the rollout, we can calculate gradients with respect to the policy parameters. Fill the missing part in ```accumulate_gradient```. Don't forget to call ```.backward()``` for every log probability to minimize the negative log-likelihood. Note that, we will not update the parameters in this method.\n",
    "\n",
    "Now, the only part remaining is ```box2d.py``` file. Fill the missing parts in the ```PolicyNet``` class.\n",
    "\n",
    "After the implementation is completed, you can run experiments. Don't forget to tune the hyperparameters as they affect the performance of the training.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "We will be running two experiments each contains 5 runs as mentioned previously.\n",
    "\n",
    "- Run REINFORCE in CartPole-v1 (Reach at least +400 mean reward for the last 20 episodes)\n",
    "- Run REINFORCE in LunarLander-v2 (Reach at least +100 mean reward for the last 20 episodes)\n",
    "\n",
    "\n",
    "By default, the writer logs the mean reward of the last 20 episodes. This can be changed by overwriting --log-window command-line argument.\n",
    "\n",
    "Plot these results (2 Plots). Also **keep** the ```.csv``` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C Implementation\n",
    "\n",
    "A2C is a synchronized version of the popular A3C algorithm. The synchronization is done via a wrapper that runs multiple environments in parallel. We use the wrapper provided in ```pg/ac2/vecenv.py``` file.\n",
    "\n",
    "#### Model\n",
    "\n",
    "Fill the missing part in ```forward``` method in ```pg/ac2/model.py```. You may want to check the return value of the network in ```pg/ac2/box2d.py``` to know what to expect when you call ```self.network(.)``` in the ```forward``` method.\n",
    "\n",
    "Start filling from the ```forward``` method in ```pg/ac2/model.py```. You may want to use ```torch.distributions.categorical.Categorical``` to represent the categorical distribution using the ```logit``` output from the ```network``` call. Please check out the Categorical function from the torch documentation.\n",
    "\n",
    "Next part to fill is ```accumulate_gradient``` where the Rollout (Rollout object) is used to calculate loss per transition in the rollout. Since we are using a baseline now, you need to calculate the n-step target. Observe that Rollout class is a namedtuple with the attributes as ```list``` and ```target_value```. Here ```list``` is the list of Transitions (also defined in the class definition) while ```target_value``` is/must be the value of the next_state at the last transition (simply, it is the (n+1)th state's value). Similar to REINFORCE, backpropagate the loss but do not update it.\n",
    "\n",
    "The only remaining method to fill is ```learn``` method where all the training happens. Start with the first missing part where you need to gather a rollout. Please, read the comments under the missing part. Below that, we have the writer object. It is the same writer object as in REINFORCE.\n",
    "\n",
    "The last part to fill in ```learn``` lies after the rollout loop. At this point, we have a rollout full of transitions. You must calculate ```target_value``` for the nth next state here. After you have it create a Rollout object with the list of Transitions and target value. Note that, we used lowercase rollout to denote a list of Transitions and uppercase Rollout to denote Rollout namedtuple. After creating a Rollout object, you can call the ```accumulate_gradient``` method with the rollout. Update the parameters by calling ```step``` function of the optimizer and you are done.\n",
    "\n",
    "#### Box2d\n",
    "\n",
    "Now we need to create a neural network that represents the policy and the value. Unlike before, we will be using a recurrent layer (GRU layer). That is why we have additional tensors like ```gru_hx``` in the ```learn``` method. We assume a familiarity with the recurrent networks.\n",
    "\n",
    "Start filling the ```__init__``` method. You can use separate networks to represent policy and value or a shared feature network and two separate head layers. Next, fill the ```forward``` method. Remember to return policy logits (no nonlinearity), value (no nonlinearity), and the hidden vector for the GRU layer. The Categorical distribution will be created in the ```A2C``` class and not in the ```network```.\n",
    "\n",
    "When you complete all the implementations mentioned above you can start experimenting (and debugging) with CartPole and LunarLander environments.\n",
    "\n",
    "#### Experiments\n",
    "\n",
    "We will be running two experiments each contains 5 runs as mentioned previously.\n",
    "\n",
    "- Run A2C in CartPole-v1 (Reach at least +400 mean reward for the last 20 episodes)\n",
    "- Run A2C in LunarLander-v2 (Reach at least +100 mean reward for the last 20 episodes)\n",
    "\n",
    "\n",
    "By default, the writer logs the mean reward of the last 20 episodes. This can be changed by overwriting --log-window command-line argument.\n",
    "\n",
    "Plot these results (2 Plots). Also **keep** the ```.csv``` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pong\n",
    "\n",
    "Pong is a visual domain so you may want to use convolutional layers (not mandatory). Other than that, there is only a handful of differences between the Pong network and the box2d network. We are using the same environment wrappers that are designed specifically for the Pong environment. You don't have to use a GPU in this experiment as it learns very fast given the implementation is correct.\n",
    "\n",
    "#### Experiments\n",
    "\n",
    "We will be running a **single run**.\n",
    "\n",
    "- Run A2C in Pong (Reach at least +10 mean reward for the last 20 episodes)\n",
    "\n",
    "By default, the writer logs the mean reward of the last 20 episodes. This can be changed by overwriting --log-window command-line argument.\n",
    "\n",
    "Plot the result (1 Plot). Also **keep** the ```.csv``` file.\n",
    "\n",
    "Note that, you need to save the model parameters for the trained agent. It will be tested in homework evaluations using the ```test``` function given in the ```model.py``` file. If your model file is too large to submit via Ninova please give a google drive link below. Do not forget to keep the link until the homework is graded and **please** do not modify the file in the drive after the submission deadline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pong run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the Google Drive link if necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
