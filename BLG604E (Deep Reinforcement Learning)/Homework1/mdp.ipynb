{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "Here we will be studying Markov Decision Processes. But before that, let's remember Random Processes and Random Sequences from the probability courses. In Reinforcement Learning (RL) we always deal with discrete-time random  processes, which are formally named as random sequences, and we simply call them random processes.\n",
    "\n",
    "RL is fundamentally built on the top of Markov Decision Processes that satisfy the Markov property. But why? One of the crucial aspects of RL is that we deal with sequential problems in which the decisions are made sequentially over discrete time. But so far, we don't need to have Markov property. In the following lectures and implementations, you will see why assuming this property is important and makes everything simpler. Although not every problem fundamentally obeys the Markov property, either they can be converted into one or the assumption is sufficient to solve the decision problem.\n",
    "\n",
    "- **Question 1)**\n",
    "    Write down the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov property is the condition when a stochastic process' all possible future states depends on only the present state. A process with this property is called \"Markov Process\".\n",
    "\n",
    "Markov property: $\n",
    "p(s_{t+1},r_{t}|s_t,a_t,s_{t-1},a_{t-1},r_{t-1}, \\ldots) = \n",
    "p(s_{t+1},r_{t}|s_t, a_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RL, the objective is to maximize the possible gain, score or objective. Decisions are made at every timestep and a reward is observed after the transition from one state (where the decision is made) to another (most of the time randomly). Since the model we use is sequential, we are interested in maximizing the cumulative reward. We call this the credit assignment problem and it is one of the key aspects of RL. We don't initially know which actions (decisions) are crucial for obtaining a high cumulative reward. Solving this problem is very important for building good decision making systems in RL.\n",
    "\n",
    "#### Modelling the World\n",
    "\n",
    "As stated before, we use MDPs to model environments. At every state, an agent takes an action and the environment transitions the agent into another state. At this transition, the agent gains a reward and this procedure repeats until the terminal state is reached in a finite horizon MDP. MDP and the agent produce a sequence of State $S$, Action $A$ and Reward $R$ at every timestep.\n",
    "\n",
    "$$ S_0, A_0, R_0, S_1, A_1, R_1, ... ,S_n, A_n, R_n $$\n",
    "\n",
    "This sequence/trajectory is known as a Markov Chain. Remember that MDP alone cannot produce a Markov Chain and for a stationary MDP, each agent may produce a different Markov Chain.\n",
    "\n",
    "#### Evaluation of the decision\n",
    "\n",
    "We'd like to evaluate the quality of the decision, so that we can increase or decrease the likeliness of taking the actions. In other words, we need to assign the credit to the decision. One of the simple yet useful way to perform credit assignment is to summate all the future rewards( also known as the __return__) starting from the current state.  \n",
    "\n",
    "$$ G_{t_0} = \\sum_{t = t_0}^{\\inf} \\gamma^{t - t_0} r_t$$\n",
    "\n",
    "- **Question 2)**\n",
    "    Write down the possible reasons for using gamma as a discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factors are associated with time horizons. Longer time horizons have have much more variance as they include more irrelevant information, while short time horizons are biased towards only short-term gains.\n",
    "\n",
    "The discount factor essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov property allows us to use the Bellman equation to calculate the return G.\n",
    "\n",
    "- **Question 3)**\n",
    "    Write down the Bellman equation. How can you use it to calculate the returns faster given that the Markov property is satisfied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Bellman eq. would be written as;\n",
    "\n",
    "$$V^\\pi(s_t)=\\max_{a_t}\\big(R(a_t,s_t)+\\gamma\\sum_{s_{t+1}}P(s_t, a_t, s_{t+1})V(s_{t+1})\\big)$$\n",
    "\n",
    "We can adapt it for calculating cumulative return G;\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bellman equations we can assign memory for each state. We will call it the value of a state.\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathbb{E}[G_t | S_t = s]$$\n",
    "\n",
    "Remember that changing your decisions at a state will change the transition probabilities and that will affect returns as well. That is why for each policy $\\pi$, that we can follow, we have different returns and values. Essentially, __each Markov Chain has its own Value function__.\n",
    "\n",
    "Using the Bellman equation, we can calculate a value for each state we visit. This allows us to optimize the policy we follow by simply choosing an action that will lead us to a state with the highest value. However, sometimes, the environment doesn't transition us into the state that we aim for. We, therefore, model the transition dynamics stochastically by $T(s'|s, a)$.\n",
    "\n",
    "- **Question 4)**\n",
    "    The value function is an expectation of the return $G$ starting from state $s$. Under which random variables that the expectation is taken over? State their density functions (such as $p(x | y)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are taking expectations over $G_t$ given $S_t$.\n",
    "\n",
    "$p(S_t | S_{t-1}, a_{t-1})$\n",
    "\n",
    "$p(G_t | )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can use another expectation but this time for the decision. We will be calling it $Q$ function of a state and action pair.\n",
    "\n",
    "$$ Q^\\pi(s, a) = \\mathbb{E}[G | S_t=s, A_t=a]$$\n",
    "\n",
    "This makes the decision process easier as we can simply take the action with the highest Q value.\n",
    "\n",
    "- **Question 5)**\n",
    "    Both $Q^\\pi(s, a)$ function and $V^\\pi(s)$ function are expectations of the return. Write down the $Q^\\pi(s, a)$ function in terms of $V^\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^\\pi(s,a) = R(s,a) + \\gamma V^\\pi(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 6)**\n",
    "    Let's define an initial state distribution $\\rho(s)$. Assume that you know the transition dynamics of the model and you want to choose from two policies. How can you compare the performance of the two policies $\\pi_0$ and $\\pi_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through $\\pi_0$ and $\\pi_1$ in $argmax$ operation. The formula will give us the policy with the higher $Q$ value as the optimal policy. We used an expectation operator, because our initial state comes from a distribution.\n",
    "\n",
    "$$\\pi^* = arg\\max_\\pi \\mathop{\\mathbb{E}}[Q^\\pi(s,\\pi(s))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, we can update a policy using the values (either $Q$ or $V$) if we know the transition dynamics and the reward function. A policy $\\pi$ is a better (or equal) policy iff for every state it has a higher or equal value. We define an optimal policy such that at every state, it has at least equal value comparing other policies.\n",
    "\n",
    "- **Question 7)**\n",
    "    The optimal value is denoted by $V^*$. Write down the optimal value in terms of $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^* = \\max_\\pi V^\\pi(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Obtaining optimal value is somewhat straightforward, however, optimizing the policy requires iterative methods. Throughout the homework, we will be implementing these methods both under known transition dynamics and unknown transition dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Lets initialize an environment where we can practice with value functions, returns and policies. The first step is to build the environment. Luckily we have a few environments in the repository(src/env). It is **MazeWorld** environment built with pycolab package.\n",
    "\n",
    "Mazeworld environment is a **gym** like environment where you need to call ```reset()``` to initiate the enviroment. Use ```step(action)``` to iterate one step with the given action. States in Mazeworld are the position of the player (x, y). There are 4 possible actions:\n",
    "- Up: 0\n",
    "- Down: 1\n",
    "- Right: 3\n",
    "- Left: 2\n",
    "\n",
    "In order to render the environment, you need to call ```init_render()``` to initiate renderer. We use ipycanvas to render the board.\n",
    "\n",
    "Note that the maximum time length is 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlztrk/anaconda3/envs/drl/lib/python3.6/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl_hw1.env import MazeWorld\n",
    "import time\n",
    "\n",
    "worldmap = [\n",
    "     \"#######\",\n",
    "     \"#    @#\",\n",
    "     \"#     #\",\n",
    "     \"#     #\",\n",
    "     \"#     #\",\n",
    "     \"#P    #\",\n",
    "     \"#######\"]\n",
    "\n",
    "env = MazeWorld(worldmap=worldmap, cell_size=40)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a policy so that we can calculate returns and values.\n",
    "\n",
    "**Question 8)** First, run the cells below. Then, change the dumb policy so that it can reach the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768cd0967b3d47dd849ad9e4cab7475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dumb_policy = lambda x, y: 0\n",
    "\n",
    "def dumb_policy(x, y):\n",
    "    if y<5:\n",
    "        return 3\n",
    "    elif x>1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "    \n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "\n",
    "while not done and step_count < 20:\n",
    "    action = dumb_policy(*state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "    step_count += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a single example of the environments. They all have similar structure, ```step```, ```reset``` and ```render```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
