{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP\n",
    "\n",
    "Dynamic programming exploits the Markov property to find an optimal policy. The main idea in Dynamic Programming (DP) is to use memory to store values that we calculated instead of recalculating them over and over. DP methods are guaranteed to find optimal policy and values. However, these methods require full knowledge of the MDP.\n",
    "\n",
    "Recall the MazeWorld environment from the MDP notebook. It is fully deterministic. But this is not the case for most of the problems. In this notebook, we will be working on StochasticMaze. Which has a few properties as follows:\n",
    "\n",
    "- The agent stays (in current state) when the chosen action leads to a non-passable state.\n",
    "- Transitions in the environments are stochastic.\n",
    "- Intended action is chosen 70% of the time and one of the remaining 3 actions is chosen with 10% probability\n",
    "- The environment terminates when the state with the goal is reached.\n",
    "- Rewards obtained during a transition depends only on the next state.\n",
    "\n",
    "In order to use DP methods, we need to build a map of transitions. This map has a structure as given below:\n",
    "\n",
    "\n",
    "```\n",
    "transition_map[state][action] -> [(probability, neighboor_state, reward, termination), ...]\n",
    "\n",
    "```\n",
    "\n",
    "**Note** that, each state-action pair points to a list of possible transitions and within a transition of 4 tuples we have a probability of the transition, next state, reward, termination(binary) values. Since the agent can only transition into 4 different states at max, the length of the list is 4 for each state-action pair.\n",
    "\n",
    "Let's render the initial board of the environemnt. \n",
    "<span style=\"color:#989898\">Dark gray cells</span> are impassable while\n",
    "<span style=\"color:#DADADA\">light gray cells</span> are passable empty cells.\n",
    "<span style=\"color:#00B8FA\">Blue cell</span> represents the agent and\n",
    "<span style=\"color:#DADA22\"> golden cell</span> reprensents the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "CELL_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlztrk/anaconda3/envs/drl/lib/python3.6/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b8f4cd26f44b789ad74df159813b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rl_hw1.env.mazeworld import StochasticMaze\n",
    "\n",
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "state = env.reset()\n",
    "board = env.board.copy()\n",
    "state\n",
    "env.init_render() # In order to visualize this must be the last line of the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the board of the game using ```env.board```. Note that the game must be initiated at least once.\n",
    "\n",
    "**Question 1)** In the ```rl_hw1/dp/transition.py``` module, implement ```make_transition_map``` as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated that values in a Markov chain(the combination of MDP and a policy) can be evaluated iteratively. This procedure is called **Policy Evaluation**. In order to update the policy, we need to know the values at each state so that we can improve it. Since we know everything about the MDP(transition dynamics), we can iteratively compute the optimal value for a fixed policy.\n",
    "\n",
    "Policy evaluation at a state $s$ can be simply calculated by following the equation shown below:\n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$ \n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s]$$ \n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma V^\\pi(S_{t+1}) | S_t = s]$$\n",
    "$$ V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a)\\big[r + \\gamma V^\\pi(s')\\big]$$\n",
    "\n",
    "Uppercase letters denote random variables while lowercase letters denote scalar values.\n",
    "\n",
    "**Question 2)** In the third equation, $ G_{t+1} $ replaced with $V^\\pi(S_{t+1})$. Why not it replaced with $V^\\pi(s')$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with the expectations while calculating transition results, we can't strictly define the next state as a scalar value. We must define the next state in random variable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3)** In the ```rl_hw1/dp/methods.py``` module, implement ```one_step_policy_eval``` in the ```DPAgent``` class.\n",
    "\n",
    "Let's test your implementation to see if we can calculate the values. We will be visualizing the values that will be drawn on the right side of the canvas. Remember that the initial policy is random so, we expect to see some sort of diffusion. Let's iterate the ```one_step_policy_eval``` 40 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4)** To test your policy evaluation code we need a policy. So, in the ```rl_hw1/dp/methods.py``` module, implement ```policy``` in the ```DPAgent``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864913dabdb442c1870232d5445c10e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "env.init_render() # In order to visualize this must be the last line of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_hw1.dp import make_transition_map, DPAgent\n",
    "\n",
    "tmap = make_transition_map(board)\n",
    "agent = DPAgent(4, tmap)\n",
    "# Color map for values\n",
    "cmap = cm.get_cmap(\"viridis\", 100)\n",
    "\n",
    "# Initiating a 2D value image\n",
    "values = np.zeros_like(board)\n",
    "for i in range(40):\n",
    "    agent.one_step_policy_eval(gamma=0.95)\n",
    "    # Filling the value image\n",
    "    for key, val in agent.values.items():\n",
    "        values[key] = val * 100\n",
    "    # Painting new values\n",
    "    env._renderer.draw(values, 0, CELL_SIZE*(board.shape[1]+1), cmap)\n",
    "    time.sleep(1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate updated values. The next step is to improve the policy according to updated values. We call this method **policy improvement**. At each state, we update the policy by changing action probabilities so that the policy is improved.\n",
    "\n",
    "**Question 5)** in the ```rl_hw1/dp/methods.py``` module, implement ```policy_improvement``` in the ```DPAgent``` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6)** You learned(or already knew) policy improvement and value evaluation. Why do we need to evaluate values whenever we improve the policy? Is it possible to find the optimal values(values of the optimal policy) without using policy improvement(explain your answer)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because if there are better valued state-action pairs we would want to go with that action in our policy. We need to update our policy regarding to that info, otherwise we will be stuck at the policy at the initilization step's decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything necessary to move on to Policy and Value iteration methods. The idea behind them is very simple. But before defining them, let's look at what we have. On one hand, we can evaluate the current value iteratively; on the other hand, we can instantly improve our policy to follow the high-value path. The first method that comes into mind, is to call these methods in turns. We can find the perfect values by calling ```policy_evaluation``` until convergence and improve the policy with ```policy_improvement```. This method is called **Policy Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7)** in the ```rl_hw1/dp/methods.py``` module, implement ```PolicyIteration``` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of the **Policy Iteration** is that we need to evaluate values until convergence after each ```policy_improvement``` call. It is observed that we don't need perfect values to improve the policy. Instead of evaluating the values until convergence we can call ```policy_improvement``` after each value evaluation step. Using this strategy is called **Value Iteration**.  \n",
    "\n",
    "**Question 8)** in the ```rl_hw1/dp/methods.py``` module, implement ```ValueIteration``` class.\n",
    "\n",
    "Now, let's compare these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(agent, env, x_offset, y_offset):\n",
    "    # Scroll up before it starts\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Initiating a 2D value image\n",
    "    values = np.zeros_like(board)\n",
    "    time.sleep(1.0)\n",
    "    for i in range(20):\n",
    "        agent.optimize(0.95)\n",
    "        # Filling the value image\n",
    "        for key, val in agent.values.items():\n",
    "            values[key] = val * 100\n",
    "        # Painting new values\n",
    "        env._renderer.draw(values, x_offset, y_offset, cmap)\n",
    "        time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_loop(agent, env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        time.sleep(1/10)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dd7b2de1d746eaa0dc9f9010f0a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rl_hw1.dp import make_transition_map, PolicyIteration, ValueIteration\n",
    "\n",
    "CELL_SIZE = 21\n",
    "tmap = make_transition_map(board)\n",
    "pi_agent = PolicyIteration(tmap)\n",
    "vi_agent = ValueIteration(tmap)\n",
    "# Color map for values\n",
    "cmap = cm.get_cmap(\"viridis\", 100)\n",
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "env.init_render() # In order to visualize this must be the last line of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteartion\n",
    "training_loop(pi_agent, env, 0, CELL_SIZE*(board.shape[1]+1))\n",
    "running_loop(pi_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteartion\n",
    "training_loop(vi_agent, env, 0, CELL_SIZE*(board.shape[1]*2+2))\n",
    "running_loop(vi_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see both having reasonable value diffusions and near-optaimal policies(can reach the goal) then **well done!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
