{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMNIST Example with your own implementation (BONUS)\n",
    "\n",
    "Now, in this part, you will work with Kuzushiji-MNIST data (https://github.com/rois-codh/kmnist) for character classification. \n",
    "The images contain one of the 10 characters in Kuzusjihi(cursive Japanese) Alphabet.\n",
    "Use an appropriate loss function.\n",
    "\n",
    "You should build a ConvNet architecture including all layers such as Conv2d, Maxpool, Dropout, and BatchNorm. You are free to design the layers as you like.\n",
    "\n",
    "IMPORTANT: You are NOT allowed to use sklearn or any other implementations for the learning part . You are ALLOWED ONLY TO USE your own implementation from the above steps.\n",
    "\n",
    "\"KMNIST Dataset\" (created by CODH), adapted from \"Kuzushiji Dataset\" (created by NIJL and others), doi:10.20676/00000341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blg561.layer import layers_with_weights\n",
    "from blg561.layer import layer\n",
    "from blg561.layer.optimizers import SGDWithMomentum, VanillaSDGOptimizer\n",
    "from blg561.layer.model import Model\n",
    "from blg561.checks import grad_check,rel_error\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "[[[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]\n",
      "\n",
      "\n",
      " [[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]\n",
      "\n",
      "\n",
      " [[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]\n",
      "\n",
      "\n",
      " [[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]\n",
      "\n",
      "\n",
      " [[[-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   ...\n",
      "   [-0.90625   -0.4140625 -0.5390625 ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]\n",
      "   [-1.        -1.        -1.        ... -1.        -1.\n",
      "    -1.       ]]]]\n"
     ]
    }
   ],
   "source": [
    "# create your own dataloader\n",
    "def load_kmnist():\n",
    "    \n",
    "    X_train = np.load(\"./kmnist/kmnist-train-imgs.npz\")['arr_0']\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_test = np.load(\"./kmnist/kmnist-test-imgs.npz\")['arr_0']\n",
    "    X_test = np.expand_dims(X_test, axis=1)\n",
    "    y_train = np.load(\"./kmnist/kmnist-train-labels.npz\")['arr_0']\n",
    "    y_test = np.load(\"./kmnist/kmnist-test-labels.npz\")['arr_0']\n",
    "    \n",
    "    X_val = X_test[0:3000]\n",
    "    X_test = X_test[3000:10000]\n",
    "    y_val = y_test[0:3000]\n",
    "    y_test = y_test[3000:10000]\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    \n",
    "    return X_train[:100]/128-1, X_test[:100], X_val[:100], y_train[:100], y_test[:100], y_val[:100]\n",
    "\n",
    "xtr, xte, xval, ytr, yte, yval = load_kmnist()\n",
    "\n",
    "print(xtr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "In below, we provide an example model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train your model with the data and show the results as Loss Curves and Accuracy for Test in a Confusion Matrix (You can use scikit-learn's confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "layers = [\n",
    "          layers_with_weights.Conv2d(in_size=1, out_size=32, kernel_size=3, stride=1, padding=1),\n",
    "          layer.BatchNorm(32),\n",
    "          layer.ReLU(),\n",
    "          layers_with_weights.Conv2d(in_size=32, out_size=32, kernel_size=3, stride=1, padding=1),\n",
    "          layer.BatchNorm(32),\n",
    "          layer.ReLU(), \n",
    "          layer.MaxPool2d(pool_height=2, pool_width=2, stride=1),\n",
    "          layers_with_weights.Conv2d(in_size=32, out_size=64, kernel_size=3, stride=1, padding=1),\n",
    "          layer.BatchNorm(64),\n",
    "          layer.ReLU(),\n",
    "          layer.MaxPool2d(pool_height=2, pool_width=2, stride=1),\n",
    "          layer.Flatten(), \n",
    "          layers_with_weights.AffineLayer(43264, 128), \n",
    "          layer.BatchNorm(128),\n",
    "          layer.ReLU(),\n",
    "          layers_with_weights.AffineLayer(128,10),\n",
    "          layer.Softmax()\n",
    "        ]\n",
    "\n",
    "model(layers) # Load layers to model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "def xavier_init(model): ## execute xavier init on the model and return it\n",
    "    for i in range(len(model.layers)): ## iterate over layers\n",
    "        try:\n",
    "            model.layers[i].W = np.random.randn(model.layers[i].W.shape) * np.sqrt(2.0 / model.layers[i].W.shape[-1]) ## xavier init\n",
    "        except:\n",
    "            continue\n",
    "    return model\n",
    "\n",
    "def get_mini_batches(X, y, batch_size):\n",
    "    random_idxs = random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "    X_shuffled = X[random_idxs]\n",
    "    y_shuffled = y[random_idxs]\n",
    "\n",
    "    mini_batches = [(X_shuffled[i*batch_size:(i+1)*batch_size], y_shuffled[i*batch_size:(i+1)*batch_size]) for i in range(X.shape[0] // batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "def get_mb_len(X, y, batch_size):\n",
    "    random_idxs = random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "    X_shuffled = X[random_idxs]\n",
    "    y_shuffled = y[random_idxs]\n",
    "    mini_batches = [(X_shuffled[i*batch_size:(i+1)*batch_size], y_shuffled[i*batch_size:(i+1)*batch_size]) for i in range(X.shape[0] // batch_size)]\n",
    "    return len(mini_batches)\n",
    "\n",
    "def whole_train_w_batch(model, x_train, y_train, x_val, y_val, x_test, y_test, lr=1e-2, reg=1e-3, epochs=200, verbose=False, optim=\"sgd\", batch=500):\n",
    "\n",
    "    model = xavier_init(model)\n",
    "    predictions = np.ones(150)\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    val_accs = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    W_affine_layers = []\n",
    "    regularization_strength = reg\n",
    "    n_epochs = epochs\n",
    "    print_every = 1\n",
    "    test_every = 1\n",
    "    mb_len = get_mb_len(x_train, y_train, batch)\n",
    "\n",
    "    if (optim==\"sgd\"):\n",
    "        optimizer = VanillaSDGOptimizer(model, lr=lr, regularization_str=regularization_strength)\n",
    "    elif (optim==\"sgdm\"):\n",
    "        optimizer = SGDWithMomentum(model,lr=lr, regularization_str=regularization_strength, mu=.5)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        i = 0\n",
    "        val_acc_per_epoch = 0\n",
    "        tra_acc_per_epoch = 0\n",
    "        tes_acc_per_epoch = 0\n",
    "\n",
    "        for xybatch in tqdm(get_mini_batches(x_train, y_train, batch), \"Epoch \" + str(epoch), leave=False):\n",
    "            xbatch = xybatch[0]\n",
    "            ybatch = xybatch[1]\n",
    "            softmax_out = model.forward(xbatch)\n",
    "            predictions = np.argmax(softmax_out, axis=1)\n",
    "            print(np.mean(predictions == ybatch))\n",
    "            tra_acc_per_epoch += np.mean(predictions == ybatch)\n",
    "            tra_loss = layer.loss(softmax_out, ybatch)\n",
    "            train_losses.append(tra_loss)\n",
    "            model.backward(ybatch)\n",
    "            #print(\"bw tamam\")\n",
    "            optimizer.optimize()\n",
    "            #print(\"optim tamam\")\n",
    "\n",
    "            if epoch % test_every == 5:\n",
    "                softmax_out = model.forward(x_val)\n",
    "                predictions = np.argmax(softmax_out, axis=1)\n",
    "                val_loss = layer.loss(softmax_out, y_val)\n",
    "                val_acc_per_epoch += np.mean(predictions == y_val)\n",
    "                val_losses.append(val_loss)\n",
    "                softmax_out = model.forward(x_test)\n",
    "                predictions = np.argmax(softmax_out, axis=1)\n",
    "                tes_loss = layer.loss(softmax_out, y_test)\n",
    "                tes_acc_per_epoch += np.mean(predictions == y_test)\n",
    "                test_losses.append(tes_loss)\n",
    "                #print(\"test tamam\")\n",
    "                \n",
    "        tra_acc_per_epoch /= mb_len\n",
    "        tes_acc_per_epoch /= mb_len\n",
    "        val_acc_per_epoch /= mb_len\n",
    "\n",
    "        train_accs.append(tra_acc_per_epoch)\n",
    "        test_accs.append(tes_acc_per_epoch)\n",
    "        val_accs.append(val_acc_per_epoch)\n",
    "        #print(\"loop tamam\")\n",
    "        if (epoch % print_every == 0) and verbose:\n",
    "            print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, tra_loss, tra_acc_per_epoch))\n",
    "            #print(\"Epoch: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch, val_loss, val_acc_per_epoch))\n",
    "            #print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, tes_loss, tes_acc_per_epoch))\n",
    "        time.sleep(0.5)\n",
    "    return train_losses, test_losses, val_losses, train_accs, test_accs, val_accs, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:   8%|▊         | 1/12 [00:09<01:43,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  17%|█▋        | 2/12 [00:18<01:34,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  25%|██▌       | 3/12 [00:28<01:25,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  33%|███▎      | 4/12 [00:37<01:15,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  42%|████▏     | 5/12 [00:47<01:06,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  50%|█████     | 6/12 [00:56<00:56,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0:  58%|█████▊    | 7/12 [01:06<00:47,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3b4844a722ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhole_train_w_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgdm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1b261d971452>\u001b[0m in \u001b[0;36mwhole_train_w_batch\u001b[0;34m(model, x_train, y_train, x_val, y_val, x_test, y_test, lr, reg, epochs, verbose, optim, batch)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtra_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtra_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m#print(\"bw tamam\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ÖDEV/blg561/layer/model.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# print(ix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerWithWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mdprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mdprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ÖDEV/blg561/layer/layers_with_weights.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dprev)\u001b[0m\n\u001b[1;32m    171\u001b[0m                         \u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfidx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpadded_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFW\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## calculating weight gradient for the given filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                         \u001b[0mdx_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFW\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfidx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## creating padded version of the value gradient matrix (padded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses2, a, val_losses2, b, c, d, model = whole_train_w_batch(model, xtr, ytr, xval, yval, xte, yte, batch=8, epochs=15, optim=\"sgdm\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
